独自のデータセットを構築する：イギリスにおけるナイフ犯罪の記録
Written by Caelainn Barr

概要

調査とパワフルなストーリーテリングのためのデータセットの構築

キーワード：データ・ジャーナリズム、犯罪、説明責任、人種、イギリス、データベース
2017年の初め、ガーディアンのニュースルームで、ゲイリー・ヤングとダミアン・ゲイルという同僚2人が声をかけてきました。彼らはイギリスのナイフ犯罪を調査したいと考えていました。
被害者の死を詳細に記した記事や、容疑者の追跡調査のフォローアップ、犯人の裁判や有罪判決に関する記事には事欠かないものの、全ての殺人事件を俯瞰的に見ている人はいなかったのです。
私はまず、「近年、ナイフで殺された子供や若者は何人いるのだろう」と考えました。簡単な質問に思えましたが、データを見つけようとすると、誰も把握していないことがわかりました。
データはどこかにあるのだが、公開されていないのです。私には二つの選択肢がありました。あきらめるか、自分でアクセスし、構築し、検証できるものをもとに、ゼロからデータセットを作るか。私はデータセットを作ることにしました。

独自のデータセットを構築する理由
データ・ジャーナリズムは、既存のデータセットだけに基づく必要はありません。独自のデータ（セット）を作成することには大きな意味があります。
日常的に公開されていないデータや、場合によっては収集さえされていないデータには、豊富な情報が含まれています。
独自のデータ・セットを作成すれば、独自の情報セット、つまり1回限りのソースを作成し、ストーリーを探ることができます。
データとそれに続くストーリーは独占的なものになる可能性が高く、他の記者が簡単には見つけられないストーリーを見つける競争力を与えてくれます。
また、独自のデータセットは、専門家や政策立案者が気づかなかったトレンドを見出すのにも役立ちます。
データはジャーナリズムにおける情報源です。ジャーナリズムでデータを活用する基本的な姿勢は、構造的な思考です。
データを最大限に活用するためには、プロジェクトの最初の段階で、ジャーナリストは構造的に考える必要があります。自分が伝えたいストーリーは何か、それを伝えるためには何が必要か。
記事のためのデータセットをうまく構築するには、ストーリーを構造的に考え、ジャーナリストとしての好奇心を持ってあらゆる情報源に問い合わせをすることが重要です。
独自のデータセットを構築することは、構造的思考、計画的なストーリーテリング、創造的な方法でのデータ検索など、データ・ジャーナリズムの重要なスキルの多くを含んでいます。また、プログラミングのスキルがなくてもできるので、比較的参入障壁が低いのも特徴です。
スプレッドシートへの入力と表のソートができれば、データ・ジャーナリズムの基本的なスキルを身につけることができます。
データ・ジャーナリズムが簡単というわけではありません。しっかりとした、徹底したデータプロジェクトは、非常に複雑で時間のかかる作業になりますが、いくつかの重要なスキルを身につければ、データをストーリーテリングに活用する強固な基盤を築くことができます。

独自のデータセットを構築するためのステップ
何が必要かを考えましょう。分析に使うデータを作成または収集するのに求められる最初のステップは、何が必要で、それが入手できるかどうかを評価することです。
どんなプロジェクトでも、最初にストーリーメモを作成する価値があります。このメモには、ストーリーが何を伝えようとしているのか、どこにデータがあるのか、それを見つけるのにどれくらいの時間がかかるのか、どこに落とし穴があるのかをスケッチします。
このメモがあれば、作業にどれくらいの時間がかかるか、その努力に見合う成果が得られるかどうかを評価するのに役立ちます。また、このメモは、作業の途中で見返すことができます。

Think of the top line. At the outset of a data-driven story where the data does not exist you should ask what the top line of the story is. It’s essential to know what the data should contain as this sets the parameters for what questions you can ask of the data. This is essential as the data will only ever answer questions based on what it contains. Therefore, to make a data set that will fulfil your needs, be very clear about what you want to be able to explore and what information you need to explore it.

Where might the data be held? The next step is to think through where the data may be held in any shape or form. One way to do this is to retrace your steps. How do you know there is a potential story here? Where did the idea come from and is there a potential data source behind it?

Research will also help you clarify what exists, so comb through all of the sources of information that refer to the issue of interest and talk to academics, researchers and statisticians who gather or work with the data. This will help you identify shortcomings and possible pitfalls in using the data. It should also spark ideas about other sources and ways of getting the data. All of this preparation before you start to build your data set will be invaluable if you need to work with difficult government agencies or decide to take another approach to gathering the data.

Ethical concerns. In planning and sourcing any story we need to weigh up the ethical concerns and working with data is no different. When building a data set we need to consider if the source and method we’re using to collect the information is the most accurate and complete possible.

This is also the case with analysis—examine the information from multiple angles and don’t torture the data to get it to say something that is not a fair reflection of the reality. In presenting the story be prepared to be transparent about the sourcing, analysis and limitations of the data. All of these considerations will help build a stronger story and develop trust with the reader.

Get the data. Once a potential source has been identified, the next step is to get the data. This may be done manually through data entry into a spreadsheet, transforming information locked in PDFs into structured data you can analyze, procuring documents through a human source or the Freedom of Information Act (FOIA), programming to scrape data from documents or web pages or automating data capture through an application programming interface (API).

Be kind to yourself! Don’t sacrifice simplicity for the sake of it. Seek to find the most straightforward way of getting the information into a data set you can analyze. If possible, make your work process replicable, as this will help you check your work and add to the data set at a later stage, if needed.

In obtaining the data refer back to your story outline and ask, will the data allow me to fully explore this topic? Does it contain the information that might lead to the top lines I’m interested in?

Structure. The key difference between information contained in a stack of text-based paper documents and a data set is structure. Structure and repetition are essential to building a clean data set ready for analysis.

The first step is to familiarize yourself with the information. Ask yourself what the material contains—what will it allow you to say? What won’t you be able to say with the data? Is there another data set you might want to combine the information with? Can you take steps in building this data set which will allow you to combine it with others?

Think of what the data set should look like at the end of the process. Consider the columns or variables you would want to be able to analyze. Look for inspiration in the methodology and structure underlying other similar data sets.

Cast the net wide to begin with, taking account of all the data you could gather and then pare it back by assessing what you need for the story and how long it will take to get it. Make sure the data you collect will compare like with like. Choose a format and stick to it—this will save you time in the end! Also consider the dimensions of the data set you’re creating. Times and dates will allow you to analyze the information over time; geographic information will allow you to possibly plot the data to look for spatial trends.

Keep track of your work and check as you go. Keep notes of the sources you have used to create your data set and always keep a copy of the original documents and data sets. Write up a methodology and a data dictionary to keep track of your sources, how the data has been processed and what each column contains. This will help flag questions and shake out any potential errors as you gather and start to analyze the data.

Assume nothing and check all your findings with further reporting. Don’t hold off talking to experts and statisticians to sense–check your approach and findings. The onus to bulletproof your work is even greater when you have collated the data, so take every step to ensure the data, analysis and write-up are correct.

Case Study: Beyond the Blade

At the beginning of 2017 the data projects team alongside Gary Younge, Damian Gayle and The Guardian’s community journalism team set out to document the death of every child and teenager killed by a knife in the United Kingdom. In order to truly understand the issue and explore the key themes around knife crime the team needed data. We wanted to know—who are the young people dying in the United Kingdom as a result of stabbings? Are they young children or teenagers? What about sex and ethnicity? Where and when are these young people being killed?

After talking to statisticians, police officers and criminologists it became clear that the data existed but it was not public. Trying to piece together an answer to the question would consume much of my work over the next year.

The data I needed was held by the Home Office in a data set called the Homicide Index. The figures were reported to the Home Office by police forces in England and Wales. I had two potential routes to get the information—send a freedom of information request to the Home Office or send requests to every police force. To cover all eventualities, I did both. This would provide us with the historical figures back to 1977.

In order to track deaths in the current year we needed to begin counting the deaths as they happened. As there was no public or centrally collated data we decided to keep track of the information ourselves, through police reports, news clippings, Google Alerts, Facebook and Twitter.

We brainstormed what we wanted to know—name, age and date of the incident were all things we definitely wanted to record. But other aspects of the circumstances of the deaths were not so obvious. We discussed what we thought we already knew about knife crime—it was mostly male with a disproportionate number of Black male victims. To check our assumptions we added columns for sex and ethnicity. We verified all the f igures by checking the details with police forces across the United Kingdom. In some instances this revealed cases we hadn’t picked up and allowed us to cross-check our findings before reporting.

After a number of rejected FOI requests and lengthy delays the data was eventually released by the Home Office. It gave the age, ethnicity and sex of all people killed by knives by police force area for almost 40 years. This, combined with our current data set, allowed us to look at who was being killed and the trend over time.

The data revealed knife crime had killed 39 children and teenagers in England and Wales in 2017, making it one of the worst years for deaths of young people in nearly a decade. The figures raised concerns about a hidden public health crisis amid years of police cuts.

The figures also challenged commonly held assumptions about who knife crime affects. The data showed in England and Wales in the 10 years to 2015, one third of the victims were Black. However, outside the capital, stabbing deaths among young people were not mostly among Black boys, as in the same period less than one in five victims outside London were Black.

Although knife crime was a much-debated topic, the figures were not readily available to politicians and policy makers, prompting questions about how effective policy could be created when the basic details of who knife crime affects were not accessible.

The data provided the basis of our award-winning project which reframed the debate on knife crime. The project would not have been possible without building our own data set.
