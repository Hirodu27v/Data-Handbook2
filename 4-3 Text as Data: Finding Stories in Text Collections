データとしてのテキスト：テキスト収集からストーリーを見つける



Written by Barbara Maseda

概要

文書やスピーチのコレクションからデータストーリーを見つける方法

キーワード：データ・ジャーナリズム、非構造化データ、テキスト分析、テキストマイニング、計算報道学

ここ数年のデータ・ジャーナリズムの制作状況を見ると、非構造化データ（テキストなど）をベースにしたストーリーは、構造化データをベースにしたストーリーに比べてはるかに少ないことに気づくかもしれません。

例えば、2012～16年にデータ・ジャーナリズム・アワードにノミネートされた200件以上の作品を分析したところ、応募作品は主に地理的データと金融データに依存しており、次いでセンサー、社会・人口統計、個人データ、メタデータ、投票など頻繁に使用されるタイプのソースであることがわかりました（Loosen et al, 2020）。
つまりほとんどが構造化データです。
しかし、ニュースルームでは、ソーシャルメディアへの投稿、スピーチ、電子メール、長文の公式報告書などを扱う機会が増えており、これらのソースを処理・分析するための計算論的アプローチが重要になっています。
あなたもこの方法で作られた物語を目にしたことがあるかもしれません。例えば、トランプ大統領のツイートを統計的にまとめたものや、アメリカの選挙で大統領候補者が公の場や討論会で取り上げた主なトピックを可視化したものなどがあります。
テキストをデータとして扱うことは簡単なことではありません。ドキュメントは、フォーマット、レイアウト、内容が最も多様である傾向があるため、画一的なソリューションや、ある調査を別のドキュメントセットで再現しようとすると、複雑になります。
データのクリーニング、準備、分析は、文書のコレクションごとに大きく異なります。また、報道する価値のある主張をしたり、研究者だけでなく広く一般の人々にとっても意味のある調査結果を発表する前に、さらに人の目で確認しなければならないステップもあります。
本章では、ジャーナリストがストーリーを伝えるためにテキスト分析を利用する五つの方法を、データ・ジャーナリズムのさまざまな模範的プロジェクトを参照しながら検討します。


長さ：どれだけ書いたか、語ったか
文章や単語を数えることは、文書に対する最も単純な定量的アプローチです。昔からある作業で、ほとんどのワープロで簡単に実行できます。
学生や記者の方で、字数制限のある課題を提出したことがある方なら、特別なデータトレーニングを受けなくても理解できるでしょう。
単語数の問題点は、意味のあるベースラインに照らし合わせて結果を解釈することです。このような尺度は、温度や速度のように広く知られているわけではないので、スピーチが2,000語であるという事実から意味を導き出すのは、それほど簡単ではないかもしれません。
実際には、比較のためのベースラインやリファレンスを自分たちで作成するしかない場合が多く、それがさらなる作業につながることもあります。
場合によっては、調べようとしているイベントやスピーカーの歴史に文脈を見出すことも可能です。
例えば、2016年に行われたアメリカ大統領の年次一般教書演説の報道では、Voxは全ての歴史的な演説の長さを計算して、「オバマ大統領は史上最も言葉数の多い一般教書演説者の一人である」と判断しました（Chang, 2016）。
複数の人が話すイベントでは、それぞれの人が話す量やタイミングを、話した言葉の総数との関係で調べることができます。図17.1をご覧ください。


Figure 17.1
Figure 17.1. Visualisation of the Democratic Party debate (2015). Source: The Washington Post, https://www.washingtonpost.com/graphics/politics/2016-election/debates/oct-13-speakers/
Mentions: Who Said What, When and How Many Times
ある用語や概念が会話や文章の中で何回使われたかを数えることも、データの統計的な概要を把握するための簡単な作業です。そのためには、最も適切な要素を選択してカウントすることが重要です。
データから探ろうとする課題に応じ、"ステミング"や "レンマ化"などの正規化操作を使用して、各単語の繰り返しを数えたり、共通の語幹を持つ単語の繰り返しを数えたりすることができます。
また、"term frequency/inverse document frequency" (TF-IDF) と呼ばれる重み付けされた尺度を用いて、各文書の中で最も関連性の高い用語に焦点を当てるというアプローチもあります。次のような例があります。
頻出する用語やトピック。ガーディアンは、2016年のロンドン市長選挙で、2人の候補者が選挙前の6年間に英国議会で様々な争点となる問題（犯罪、汚染、住宅、交通など）を取り上げた回数を分析しました（Barr, 2016）。
分析対象となるトピックは、今回のようにあらかじめ決めておき、比較可能なテキストコレクションや類似のテキストコレクションに含まれる多数の関連キーワード（またはトピックに関連するキーワード群）から探索することができます。
た、検索語は類似していても、必ずしも同じではありません。例えば、同じメディアが2017年に発生した3つの異なるハリケーン（ハービー、イルマ、マリア）をどのように報道したかというFiveThirtyEightの分析を見てみましょう（Mehta, 2017）。
もう一つのアプローチは、トピック検出戦略として単にテキスト内の最も一般的な単語に注目することです。
スピーチの経時的分析。スピーチを時系列で見ることは、これまで言及されなかった、あるいは長い間取り上げられていなかったトピックを指摘する方法でもあります。
例えば、ワシントン・ポスト紙が2018年の一般教書演説を報道する際に選択したアプローチは、どの大統領がどの言葉を最初に使ったかを強調した記事でした（Fischer-Baum et al.2018）。
読者は、何百ページもの演説を読まなくても、トランプがウォルマート（2017年）やただ乗り（2019年）に言及した史上初の大統領であることを非常に素早く知ることができ、テキストデータの要約と可視化がいかに効果的であるかを示している。
省略。言及の数が少なかったり、全く言及がなかったりすることも、ニュースになることがあります。このような言及漏れは、時系列で分析できますが、加えて人や組織がある文脈で何かに言及するという期待に基づいて分析することもできます。
2016年の米大統領選挙期間中、FiveThirtyEightは、ドナルド・トランプ候補のツイートに世論調査に関するキーワードの言及が比較的少ないことを発見し、世論調査に関するツイートをやめたと報じました（Mehta & Enten, 2016）。
このような省略は、同じ発言者を長期的に監視することで発見することができます。今回のケースでは、FiveThirtyEightは数ヶ月前から、トランプが自分が勝っているように見せている世論調査をたくさんツイートしていることを発見していました（Bialik & Enten, 2015）。
これは、上述したテキスト統計の文脈化の問題を解決する方法として、テキスト分析に基づいたニュースレポートが、後にフォローアップ記事の文脈となることを示す良い例でもあります。
あるトピックの不在は、人や組織がある文脈でそのトピックについて言及するという期待に基づいて測定することもできるのです。
人、場所、名詞、動詞。自然言語処理 (NLP) ツールは、(固有表現抽出 (NER) と呼ばれるタスクを介して)固有名詞や地名、企業名などの要素の抽出 、(品詞タグ付け (POS) と呼ばれるタスクを介して) 名詞や形容詞などの単語の識別を可能にします。
先ほどのワシントン・ポストの記事では、ビジュアライゼーションに企業や宗教用語、動詞などに焦点を当てたフィルタリングが含まれています。

Comparisons

Determining how similar two or more documents are can be the starting point for different kinds of stories. We can use approximate sentence matching (also known as “fuzzy matching”) to expose plagiarism, reveal like-mindedness of public figures or describe how a piece of legislation has changed. In 2012, ProPublica did this to track changes in emails sent to voters by campaigns, showing successive versions of the same messages side by side and visualizing deletions, insertions and unchanged text (Larson & Shaw, 2012).

Classification

Text can be classified into categories according to certain predefined features, using machine learning algorithms. In general, the process consists of training a model to classify entries based on a given feature, and then using it to categorize new data.

For instance, in 2015, the Los Angeles Times analyzed more than 400,000 police reports obtained through a public records request, and revealed that an estimated 14,000 serious assaults had been misclassified by the Los Angeles Police Department as minor offenses (Poston et al., 2015). Instead of using MySQL to search for keywords (e.g., stab, knife) that would point to violent offenses—as they had done in a previous investigation covering a smaller amount of data—the reporters used machine learning classifiers (SVM and MaxEnt) to re-classify and review eight years’ worth of data in half the time needed for the first investigation, which covered one year only (Poston & Rubin, 2014). This example shows how machine learning approaches can also save time and multiply our investigative power.

Sentiment

Many journalists would recognize the value of classifying sentences or documents as positive, negative or neutral (other grading scales are possible), according to the attitude of the speaker towards the subject in question. Applications may include analyzing a topic, a hashtag or posts by a Twitter user to evaluate the sentiment around an issue, and doing similar computations on press releases or users’ comments on a website. Take, for example, The Economist’s comparison of the tone of party convention speeches by Hillary Clinton and Donald Trump (“How Clinton’s and Trump’s Convention Speeches,” 2016). Analyzing the polarity of the words used by these and previous candidates, they were able to show that Trump had “delivered the most negative speech in recent memory,” and Clinton “one of the most level-headed speeches of the past four decades.”

Becoming a “Text-Miner” Journalist

Using off-the-shelf text mining software is a good starting point to get familiar with basic text analysis operations and their outcomes (word counts, entity extraction, connections between documents, etc.). Platforms designed for journalists—such as DocumentCloud and Overview—include some of these features.5 The Google Cloud Natural Language API can handle various tasks, including sentiment analysis, entity analysis, content classification and syntax analysis.6

For those interested in learning more about text mining, there are free and open-source tools that allow for more personalized analyses, including resources in Python (NLTK, spaCy, gensim, textblob, scikit-learn) and R (tm, tidytext and much more), which may be more convenient for journalists already familiar with these languages. A good command of regular expres- sions and the tools and techniques needed to collect texts (web scraping, API querying, FOIA requests) and process them (optical character recognition or OCR, file format conversion, etc.) are must-haves as well.7 And, of course, it can be useful to obtain a grasp of the theory and principles behind text data work, including information retrieval, relevant models and algorithms, and text data visualization.8

Conclusions

The possibility of revealing new insights to audiences with and about documents, and of multiplying our capacities to analyze long texts that would take months or years to read, are good reasons to give serious consideration to the development of text analysis as a useful tool in journalism. There are still many challenges involved, from ambiguity issues—computers may have a harder time “understanding” the context of language than we humans do—to language-specific problems that can be easier to solve in English than in German, or that have simply been addressed more in some languages than in others. Our work as journalists can contribute to advancing this field. Many reporting projects could be thought of as ways of expanding the number of available annotated data sets and identifying challenges, and as new application ideas. Judging by the growing number of recent stories produced with this approach, text mining appears to be a promising and exciting area of growth in data journalism.

Footnotes

1. For more on the Data Journalism Awards, see Loosen’s chapter in this volume.

2. Data cleaning and preparation may include one or more of the following steps: Breaking down the text into units or tokens (a process known as “tokenization”); “grouping” words that share a common family or root (stemming and lemmatization); eliminating superfluous elements, such as stopwords and punctuation; changing the case of the text; choosing to focus on the words and ignore their order (a model called “bag of words”); and transforming the text into a vector representation.

3. Stemming and lemmatization are operations to reduce derived words to their root form, so that occurrences of “reporter,” “reporting” and “reported” can all be counted under the umbrella of “report.” They differ in the way that the algorithm determines the root of the word. Unlike lemattizers, stemmers strip words of their suffixes without taking into consideration what part of speech they are.

4. TF-IDF is a measure used by algorithms to understand the weight of a word in a collection. TF-IDF Weight (w, d) = TermFreq(w, d) · log (N / DocFreq(w)), where TermFreq(w, d) is the frequency of the word in the document (d), N is the number of all documents and DocFreq(w) is the number of documents containing the word w (Feldman and Sanger, 2007).

5. www.documentcloud.org, www.overviewdocs.com

6. cloud.google.com/natural-language

7. regex.bastardsbook.com

8. For further reading, see Speech and Language Processing by Daniel Jurafsky and James H. Martin; The Text Mining Handbook by Ronen Feldman and James Sanger. There are also numerous free online courses on these and associated topics.

Works cited

Barr, C. (2016, May 3). London mayor: Commons speeches reveal candidates’ differing issue focus. The Guardian. www.theguardian.com/politics/datablog/2016/may/03/london-mayor-data-indicates-candidates-differing-focus-on-issues

Bialik, C., & Enten, H. (2015, December 15). Shocker: Trump tweets the polls that make him look most like a winner. FiveThirtyEight. fivethirtyeight.com/features/shocker-trump-tweets-the-polls-that-make-him-look-most-like-a-winner/

Chang, A. (2016, January 11). President Obama is among the wordiest State of the Union speakers ever. Vox. www.vox.com/2016/1/11/10736570/obama-wordy-state-of-the-union

Feldman, R., & Sanger, J. (2007). The text mining handbook: Advanced approaches in analyzing unstructured data. Cambridge University Press.

Fischer-Baum, R., Mellnik, T., & Schaul, K. (2018, January 30). The words Trump used in his State of the Union that had never been used before. The Washington Post. www.washingtonpost.com/graphics/politics/presidential-lexicon-state-of-the-union/

How Clinton’s and Trump’s convention speeches compared to those of their predecessors. (2016, July 29). The Economist. www.economist.com/graphic-detail/2016/07/29/how-clintons-and-trumps-convention-speeches-compared-to-those-of-their-predecessors

Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing. Pearson.

Larson, J., & Shaw, A. (2012, July 17). Message machine: Reverse engineering the 2012 campaign. ProPublica. projects.propublica.org/emails/

Loosen, W., Reimer, J., & De Silva-Schmidt, F. (2020). Data-driven reporting: An on-going (r)evolution? An analysis of projects nominated for the Data Journalism Awards 2013–2016. Journalism, 21(9), 1246–1263. https://doi. org/10.1177/1464884917735691

Mehta, D. (2017, September 28). The media really has neglected Puerto Rico. FiveThirtyEight. fivethirtyeight.com/features/the-media-really-has-neglected-puerto-rico/

Mehta, D., & Enten, Ha. (2016, August 19). Trump isn’t tweeting about the polls anymore. FiveThirtyEight. fivethirtyeight.com/features/trump-isnt-tweeting-about-the-polls-anymore/

Poston, B., & Rubin, J. (2014, August 10). Times Investigation: LAPD misclassified nearly 1,200 violent crimes as minor offenses. Los Angeles Times. www.latimes.com/local/la-me-crimestats-lapd-20140810-story.html

Poston, B., Rubin, J., & Pesce, A. (2015, October 15). LAPD underreported serious assaults, skewing crime stats for 8 years. Los Angeles Times. www.latimes.com/local/cityhall/la-me-crime-stats-20151015-story.html
