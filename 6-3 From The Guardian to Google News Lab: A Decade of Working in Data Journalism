ガーディアンからグーグルニュースラボへ：データジャーナリズムに携わって10年
Written by Simon Rogers
概要

データジャーナリズムの過去10年を、その名声の高い人物の一人（サイモン・ロジャース）の職業上の旅というレンズを通して振り返ります。

キーワード：データジャーナリズム、ガーディアンのデータブログ、ウィキリークス、オープンデータ、透明性、表計算ソフト
私がジャーナリストになりたいと思ったのは、小学校1年生と2年生の間くらいのことで、まさかデータを扱うことになるとは思いもよりませんでした。
今、毎日データを使って仕事をしていると、自分がいかに幸運だったかがわかります。慎重に練られたキャリアプランの結果ではないことは確かです。ただ、適切な時期に適切な場所にいただけなのです。
このような経緯は、2009年、そして19年のデータジャーナリズムの状況について多くのことを物語っています。
シカゴ出身で、ワシントン・ポスト紙に勤務した後、近隣住民を対象としたニュース・ディスカッション・サイト「EveryBlock」を立ち上げた開発者のエイドリアン・ホロバティが、ロンドンのファリングドン・ロードにあるガーディアン紙の教育センターで、ニュースルーム向けの講演をしに来たのです。
当時、私は、オンラインを経て、科学セクションの編集を担当し、紙媒体（当時は重心が置かれていた）のニュース編集者でした。
ホロバティが、データを使ってストーリーを伝え、人々が世界を理解する手助けをすることについて話せば話すほど、私の中で何かが動きました。
可能だということだけでなく、実際にやってみることができるのです。 もしかしたら、私はデータを扱うジャーナリストになれるかもしれない。"データジャーナリスト "です。
ニュースエディターとしてグラフィックデスクを担当したことで、マイケル・ロビンソンの才能あふれるチームの中で、私の世界の見方を変えたデザイナーと仕事をする機会を得ることができました。
そして、ビジュアル分野のポートフォリオが増えるにつれて、（そこには）たくさんの数字が蓄積されていることがわかりました。
ガーディアンのオープンAPIを立ち上げたマット・マカリスターは、これを "マザーロード "と表現しています。GDPデータ、二酸化炭素排出量、政府支出データ、その他多くのデータをきれいに整理し、すべてGoogleスプレッドシートとして保存し、次に必要なときにすぐに使えるようにしました。
このデータをオープンデータとして公開したら？PDFではなく、誰でもすぐに使える、アクセス可能なデータです。これが、ガーディアンのDatablogで行ったことです。
当初は200の異なるデータセットがありました。犯罪率、経済指標、紛争地域の詳細、さらにはファッション・ウィークやドクター・フーの悪役まで。そして、データはあらゆるものに応用できるのだということに気づきました。
しかし、それはまだ奇妙な存在でした。「データエディターという職業はほとんど普及しておらず、データチームを設置している報道局はほとんどありませんでした。
ニュースの打ち合わせで「データ」という言葉を使うだけで、鼻で笑われるような状況でした。これは「まともな」ジャーナリズムではない、というわけです。
しかし、2009年はオープンデータ革命の始まりの年でした。米国政府のデータハブであるdata.govは、その年の5月にわずか47件のデータセットでスタートしました。
世界中の国や都市がオープンデータポータルを立ち上げ、活動家たちはより多くのデータへのアクセスを要求していました。
読者の協力のもと、1年足らずで何千人もの国会議員の経費をクラウドソース化できました。同じ時期に、英国政府はCOINS（Combined Online Information System）という究極の支出データセットを発表しました。
ガーディアンのチームは読者にその探索を手伝ってもらうために、インタラクティブなエクスプローラーを作りました。
そのデータから記事が生まれると、「どうすればもっと多くのデータを入手できるか」という問いが出てきました。
（答えを得るには）そんなに待ちませんでした。その答えは、スウェーデンを拠点とする新組織「ウィキリークス」が、大胆な透明性アジェンダとも言えるものを用いてもたらしました。
ウィキリークスについて今日どう思うかは別にして、この組織が近年のデータジャーナリズムの歴史に与えた影響は誇張しすぎることはないでしょう。
ここには、まずアフガニスタン、次いでイラクの戦場から、何千もの詳細な記録が大量に投げ込まれたのです。
それは巨大なスプレッドシートの形で提供され、ガーディアンの調査チームが最初に扱うには大きすぎるものでした。
ベトナム戦争の実態を明らかにしたペンタゴン・ペーパーズ（米国防総省発行の文書）よりも規模が大きく、詳細で、死傷者数、地理的位置、詳細、カテゴリーを含む事件のリストも含まれていました。
例えば、イラクでは簡易爆弾の被害が増加し、道路が危険な状態になっていることが分かりました。そして、そのデータと熟練した戦争記者の伝統的な報道技術が組み合わさったとき、世界の戦争の見え方が変わったのです。
全世界にインパクトを与えるようなコンテンツを作ることは難しくありませんでした。例えば、スプレッドシートのジオデータはマッピングに適しており、それを支援する新しい無料ツールがあったのです。
Google Fusion Tablesです。そこで私たちは、イラクで少なくとも1人の死者が出ているすべての事件を大づかみで地図化しました。
24時間以内に、1時間で作ったコンテンツが世界中で見られるようになり、ユーザーは戦場をよりリアルに感じられるようになりました。
また、データが構造化されているため、グラフィック・チームは洗練されたリッチなビジュアルを作成し、より詳細なレポートを提供することができました。
ハンドブック初版公開前年の2011年末には、「暴動を読む」プロジェクトが、1960年代にフィル・マイヤーが行ったコンピュータ支援報道の技術を、イングランドで発生した暴力事件に応用しました（Robertson, 2011）。
マイヤーは、1960年代後半のデトロイト暴動に関する報道に社会科学の手法を応用したのです。
ガーディアンのポール・ルイスが率いるチームは、その年のイングランド全土で発生した暴力事件に敷衍し、その作業の重要な部分としてデータを取り入れたのです。
これらは、一面を飾る、データに基づいたストーリーでした。
しかし、私たちが情報を消費する方法にはもう一つの変化が起きており、それは急速に進展しました。2010年以前は、健康関連の記事以外で「バイラル」という言葉を聞いた記憶がありません。
今は違います。データジャーナリズムの台頭は、ソーシャルメディアの台頭とも重なります。
私たちは世界中のユーザーに記事を売り込むのにツイートを活用し、トラフィックが増え、この種のデータに基づくストーリーを探すユーザーが増えました。
ビジュアルや数値は、数秒で何千人もの目に触れることができるのです。ソーシャルメディアはジャーナリズムを一変させましたが、データ・ジャーナリズムの拡大はニッチから主流への移行を促したのです。

For one thing, it changed the dynamic with consumers. In the past, the words of a reporter were considered sacrosanct; now you are just one voice among millions. Make a mistake with a data set and 500 people would be ready to let you know. I can recall having long (and deep) conversations on Twitter with designers around colour schemes for maps—and changing what I did because of it. Sharing made my work better.

In fact that spirit of collaboration is something that still persists in data journalism today. The first edition of this book was, after all, initially developed by a group of people meeting at the Mozilla Festival in London—and as events around data started to spring up, so did the opportunities for data journalists to work together and share skill sets.

If the Iraq and WikiLeaks releases were great initial examples of cross-Atlantic cooperation, then see how those exercises grew into pan-global reporting involving hundreds of reporters. The Snowden leaks and the Panama Papers were notable for how reporters coordinated around the world to share their stories and build off each other’s work.2

Just take an exercise like Electionland, which used collaborative reporting techniques to monitor voting issues in real time on election day. I was involved, too, providing real-time Google data and helping to visualize those concerns in real time. To this date, Electionland is the biggest single-day reporting exercise in history, with over a thousand journalists involved on the day itself. There’s a direct line from Electionland to what we were doing in those first few years.

My point is not to list projects but to highlight the broader context of those earlier years, not just at The Guardian, but in newsrooms around the world. The New York Times, the Los Angeles Times, La Nación in Argentina: Across the world journalists were discovering new ways to work by telling data-led stories in innovative ways. This was the background to the first edition of this book.

La Nación in Argentina is a good example of this. A small team of enthused reporters taught themselves how to visualize with Tableau (at that time a new tool) and combined this with freedom of information reports to kickstart a world of data journalism in Latin and South America.

Data journalism went from being the province of a few loners to an established part of many major newsrooms. But one trend became clear even then: Whenever a new technique is introduced in reporting, data would not only be a key part of it but data journalists would be right there in the middle of it. In a period of less than three years, crowdsourcing became an established newsroom tool, and journalists found data, used databases to manage huge document dumps, published data sets and applied data-driven analytical techniques to complex news stories.

This should not be seen as an isolated development within the field of journalism. These were just the effects of huge developments in international transparency beyond the setting up of open data portals. These included campaigns such as those run by Free Our Data, the Open Knowledge Foundation and civic tech groups to increase the pressure on the UK government to open up news data sets for public use and provide APIs for anyone to explore. They also included increased access to powerful free data visualization and cleaning tools, such as OpenRefine, Google Fusion Tables, Many Eyes, Datawrapper, Tableau Public and more. Those free tools combined with access to a lot of free public data facilitated the production of more and more public-facing visualizations and data projects. Newsrooms, such as The Texas Tribune and ProPublica, started to build operations around this data.

Can you see how this works? A virtuous circle of data, easy processing, data visualization, more data, and so on. The more data is out there, the more work is done with the data the greater pressure there is for more data to be released. When I wrote the piece “Data Journalism Is the New Punk” it was making that point: We were at a place where creativity could really run free (Rogers, 2012). But also where the work would eventually become mainstream.


Data can’t do everything. As Jonathan Gray (2012) wrote: “The current wave of excitement about data, data technologies and all things data-driven might lead one to suspect that this machine-readable, structured stuff is a special case.” It is just one piece of the puzzle of evidence that reporters have to assemble. But as there is more and more data available, that role changes and becomes even more important.

The ability to access and analyze huge data sets was the main attraction for my next career move.

In 2013, I got the chance to move to California and join Twitter as its first data editor—and it was clear that data had entered the vocabulary of mainstream publishing, certainly in the United States and Europe. A number of data journalism sites sprouted within weeks of each other, such as The New York Times’ Upshot and Nate Silver’s FiveThirtyEight.

Audiences out there in the world were becoming more and more visually literate and appreciative of sophisticated visualizations of complex topics. You will ask what evidence I have that the world is comfortable with data visualizations? I don’t have a lot beyond my experience that producing a visual which garners a big reaction online is harder than it used to be. Where we all used to react with “oohs and aahs” to visuals, now it’s harder to get beyond a shrug.

By the time I joined the Google News Lab to work on data journalism in 2015, it had become clear that the field has access to greater and larger data sets than ever before. Every day, there are billions of searches, a significant proportion of which have never been seen before. And increasingly reporters are taking that data and analyzing it, along with tweets and Facebook likes.3 This is the exhaust of modern life, turned around and given back to us as insights about the way we live today.

Data journalism is now also more widespread than it has ever been. In 2016, the Data Journalism Awards received a record 471 entries. But the 2018 awards received nearly 700, over half from small newsrooms, and many from across the world. And those entries are becoming more and more innovative. Artificial intelligence, or machine learning, has become a tool for data journalism, as evidenced by Peter Aldhous’ work at Buzzfeed (Aldhous, 2017).

Meanwhile access to new technologies like virtual and augmented reality open up possibilities for telling stories with data in new ways. As someone whose job is to imagine how data journalism could change—and what we can do to support it—I look at how emerging technologies can be made easier for more reporters to integrate into their work. For example, we recently worked with design studio Datavized to build TwoTone, a visual tool to translate data into sound.4

What does a data journalist at Google do? I get to tell stories with a large and rich collection of data sets, as well as getting to work with talented designers to imagine the future of news data visualization and the role of new technologies in journalism. Part of my role is to help explore how new technologies can be matched with the right use cases and circumstances in which they are appropriate and useful. This role also involves exploring how journalists are using data and digital technologies to tell stories in new ways. For example, one recent project, El Universal’s “Zones of Silence", demonstrated the use of AI in journalism, using language processing to analyze news coverage of drug cartel murders and compare them to the official data, the gap between the two being areas of silence in reporting. I helped them do it, through access to AI APIs and design resources.

The challenges are great, for all of us. We all consume information in increasingly mobile ways, which brings its own challenges. The days of full-screen complex visualizations have crashed against the fact that more than half of us now read the news on our phones or other mobile devices (a third of us read the news on the toilet, according to a Reuters news consumption study (Newman et al., 2017)). That means that increasingly newsroom designers have to design for tiny screens and dwindling attention spans.

We also have a new problem that can stop us learning from the past. Code dies, libraries rot and eventually much of the most ambitious work in journalism just dies. The Guardian’s MPs’ expenses, EveryBlock and other projects have all succumbed to a vanishing institutional memory. This problem of vanishing data journalism is already subject to some innovative approaches (as you can see from Broussard’s chapter in this book). In the long run, this requires proper investment and it remains to be seen if the community is sufficiently motivated to make it happen.

And we face a wider and increasingly alarming issue: Trust. Data analysis has always been subject to interpretation and disagreement, but good data journalism can overcome that. At a time when belief in the news and a shared set of facts are in doubt every day, data journalism can light the way for us, by bringing facts and evidence to light in an accessible way.

So, despite all the change, some things are constant in this field. Data journalism has a long history,5 but in 2009, data journalism seemed an important way to get at a common truth, something we could all get behind. Now that need is greater than ever before.

Footnotes

1. www.theguardian.com/politics/coins-combined-online-information-system

2. For more on large-scale collaborations around the Panama Papers, see Díaz-Struck, Gallego and Romera’s chapter in this volume.

3. For further perspectives on this, see the “Investigating Data, Platforms and Algorithms” section.

4. twotone.io

5. See, for example, the chapters by Anderson and Cohen in this volume.

Works cited

Aldhous, P. (2017, August 8). We trained a computer to search for hidden spy planes. This is what it found. BuzzFeed News. www.buzzfeednews.com/article/peteraldhous/hidden-spy-planes

Gray, J. (2012, May 31). What data can and cannot do. The Guardian. www.theguardian.com/news/datablog/2012/may/31/data-journalism-focused-critical

Newman, N., Fletcher, R., Kalogeropoulos, A., Levy, D. A. L., & Nielsen, R. K. (2017). Digital News Report 2017. Reuters Institute for the Study of Journalism. reutersinstitute.politics.ox.ac.uk/sites/default/files/Digital%20News%20Report%202017%20web_0.pdf

Robertson, C. (2011, December 9). Reading the riots: How the 1967 Detroit riots were investigated. The Guardian. www.theguardian.com/uk/series/reading-the-riots/2011/dec/09/all

Rogers, S. (2012, May 24). Anyone can do it. Data journalism is the new punk.The Guardian.www.theguardian.com/news/datablog/2012/may/24/data-journalism-punk

