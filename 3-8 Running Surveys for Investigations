調査のためのアンケートを行う
Written by Crina-Gabriela Boroş

概要

説明責任を果たすジャーナリズムのためにアンケートを実施する前に、まずこの記事をお読みください。必要なこと、やってはいけないこと、不完全な状況に対処する方法などを説明しています。

キーワード：統計、データ・ジャーナリズム、アンケート、説明責任

その問題は(１回限りの事例が並ぶ)逸話的なものなのか、それとも体系的なものなのか。表形式のデータ-行と列で提供される情報のしゃれた言い方-がないことに気付いたとき、あなたはそれを見極めようとします。
どうすればいいのでしょうか？
そもそも、データとは何でしょうか？オタク的な定義がたくさん出回っていて、中には人を萎縮させるものもありますが1、それらを「情報」というシンプルな概念に置き換えてみましょう。
どんな形であれ、情報を集めるには、パターンや異常値を見つける能力が必要です。つまり、ある問題を特定の方法で文書化し、体系的に収集した膨大な量の原材料が必要になるのです（記入式のフォームを考えてみてください）。
スプレッドシートを使おうが、（プログラミングの）開発環境を使おうが、アプリを使おうが、紙とペンを使おうが、構いません。
人の心に閉じ込められた思いや感情、過去の親密な体験などは、時にデータとして表現することができます。
このような貴重な情報を得るためには、そのような感情や経験を集めて整理し、自分以外の誰もアクセスできないテーブルやアーカイブ、データベースを作成するような調査を設計することが一つの方法です。
例えば、トムソン・ロイター財団（TRF）は、世界の大都市に住む女性たちが、公共交通機関での性的暴力をどのように受け止めているかを報告するプロジェクトを実施しました。
これは、この問題に対する認識を高めると同時に、比較対照を行うための調査でもありました（統計学の手法です）。
このスポットライトを提供するために、私たちは地獄巡りをしました。なぜなら、調査のような社会科学的手法には、ジャーナリストが実践に取り入れる際にも必要とされる厳密な規則があるからです。
ここでは、ジャーナリストが知っておくと便利ですが、トレーニングを受けていないことが多い、主な投票ルールを紹介します。
回答者を選ぶことはできません。代表的」と見なされるためには、社会的カテゴリー、年齢層、教育レベル、地理的エリアなど、報道すべき対象のすべての人々が回答者として含まれている必要があります。
確立された方法によれば、調査対象となる人口のサンプルは代表的である必要があります。
回答者の選択は無作為である必要があります。つまり、誰もが同じ確率で（手品のように無作為に）帽子から名前を取り出せるようにしなければなりません。
一般論的な主張をしたいのに、何の基準も方法もなく手近な人に声をかけて世論調査を行った場合は、誤解を招くようなデータを導く危険があります。
アンケートを代表的なものとするには、参加者数が一定の基準に達している必要があります。Raosoft、Survey Monkey、Survey Systemsなどは便利なオンライン計算ツールを提供しています。
経験則として、信頼度（信頼係数）は95%、誤差範囲は5%以下にしてください。回答の選択肢は、回答者が「わからない」または「確信がない」と答えられるものでなければなりません。
記者がこれらの基本的なルールに従えば、調査結果はほとんど批判されることはありません。TRFの公共交通機関の安全性に関する調査で、私たちの世論調査の方法は、社会科学の保守的なルールに従っています。
今回のテーマは、社会の仕組みを物語るような人間の共通体験を扱ったもので、国連機関からも協力の申し出がありました。光栄なことですが、ジャーナリストとしてはお断りしなければなりませんでした。
この言葉が気に入った人は、統計学のコースを受講してみるといいでしょう。
「厳密な世論調査は時に非現実的です。だからといって、世論調査をすべきでないというわけではありません」。
調査には確立された方法がありますが、それだけでは可能なこと、正当なこと、興味深いことを網羅しているわけではありません。あなたの関心事や制約、リソースに応じて、他の方法で調査を行うことができるかもしれません。
例えば、（英国の政治サイト）openDemocracyが欧州理事会加盟国47カ国の記者にニュースルーム内の商業的圧力についてインタビューを試みたとき、統計的な有意性が得られる可能性はほとんどありませんでした。
「なぜ？」と思われるかもしれません。

All respondents became whistle-blowers. Whistle-blowers need protection, including not disclosing important real demographic data, such as age or sex. We were expecting some contributions from countries where exercising freedom of speech may lead to severe personal consequences. We decided that providing personal data should not be compulsory; nor, if provided, should these data sit on a server with a company that co-owns our information.

The EU had wildly different and incomplete counts of journalists in the region, meaning establishing a country-level representative sample was tricky.

We couldn’t line up all press unions and associations and randomize respondents because membership lists are private. They also don’t include everyone, although it would have been an acceptable base as long as we were honest about our limitations. Plus, in some countries, transparency projects lead to suppression and we received expert advice in which countries we could not solicit the support of unions without attracting either surveillance or punitive consequences.

In cases like this, you needn’t throw the baby out with the bathwater.

We didn’t.

Instead, we identified what mattered for our reporting and how polling methods could be adjusted to deliver stories.

We decided that our main focus was examples of commercial pressure inside national newsrooms; whether there was a pattern of how they happened; and whether patterns matched across the region. We were also interested in the types of entities accused of image-laundering activities in the press. We went ahead and built a survey, based on background interviews with journalists, media freedom reports and focus group feedback. We included sections for open answers.

We pushed the survey through all vetted journalism organization channels. In essence, we were not randomizing, but we also had no control over who in the press took it. We also had partners—including Reporters sans frontières, the National Union of Journalists and the European Federation of Journalists—who helped spread the questionnaire.

The feedback coming through the survey was added to a unique database, assigning scores to answers and counting respondents per country, drawing comparisons between anecdotal evidence (issues reported sporadically) and systemic issues (problems reported across the board).

The open text fields proved particularly useful: Respondents used them to tip us. We researched their feedback, with an eye for economic censorship patterns and types of alleged wrongdoers. This informed our subsequent reporting on press freedom.4

Although we did publish an overview of the findings, we never released a data breakdown for the simple reason that the selection could not be randomized and country-level sample sizes were not always reached.5 But we built a pretty good understanding of how free the press is according to its own staff, how media corruption happens, how it evolves and sadly, how vulnerable reporters and the truth are.6

So, are there rules for breaking the rules?

Just a few. Always describe your efforts accurately. If you polled three top economic government advisers on a yes–no question, say so. If you interviewed ten bullying victims, describe how you chose them and why them in particular. Do not label interviews as surveys easily.

If you run a statistically significant study, have the courtesy to release its methodology.7 That affords the necessary scrutiny for your audience and experts to trust your reporting. No methodology, no trust.

Don’t be the next biggest “fake news” author. If an editor is pushing you to draw correlations based on inferences rather than precise data collection, use a language that does not suggest causality or scientific strength. Our job is to report the truth, not just facts. Do not use facts to cover up a lack of certainty over what the truth is.

Where does your story lie? In a pattern? In an outlier? Decide what data you need to collect based on this answer. Figure out where and how the data can be obtained before you decide on the most appropriate methods. The method is never the point, the story is.

If you run a survey, field-test your findings and protect your reporting against potentially problematic claims. For example, say a survey suggests that the part of the city you live in has the highest crime rate. Yet you feel safe and experienced almost weekly street violence in another neighbourhood you lived in for a year, so you may not yet trust the data. To check if you can trust your data, visit the places that you compare and contrast; talk to people on the streets, in shops, banks, pubs and schools; look at what data was collected; are residents in one area more likely to file complaints than residents in another area? What types of crime are we talking about?

Have the types of crime considered in the analysis been weighted, or does a theft equal a murder? Such “ground truthing” efforts will allow you to evaluate your data and decide to what extent you can trust the results of further analysis.

Footnotes
1. See 130 definitions of data, information and knowledge in Zins, C. (2007). Conceptual approaches for defining data, information, and knowledge. Journal of the American Society for Information Science and Technology, 58, 479–493.
2. news.trust.org/spotligh...
3. www.raosoft.com/samples..., www.surveymonkey.com/m..., www.surveysystem.com/s...
4. www.opendemocracy.net/...
5. www.opendemocracy.net/...
6. www.opendemocracy.net/...
7. news.trust.org/spotligh...

