アルゴリズムの専門性：アングルと調査方法
Written by Nicholas Diakopoulos

概要

アルゴリズムに適した精査を行うために、ジャーナリスティックなスキルと技術的なスキルが融合し、アルゴリズムに関する専門性が形成されつつあります。
キーワード：アルゴリズム、アルゴリズムの説明責任、計算論的ジャーナリズム、調査報道、アルゴリズム研究、情報の自由(FOI)
プロパブリカの社会におけるアルゴリズムを調査する取り組み「Machine Bias」シリーズは、2016年5月に始まりました。
このシリーズで最も目を引いたのは、刑事司法の判断に用いられる再犯リスク評価アルゴリズムの人種的偏りを暴いた調査・分析でした（Angwin et al.2016）。
これらのアルゴリズムは、再犯のリスクが低いか高いかに基づいて個人を採点します。州やその他の自治体は、公判前拘留、保護観察、仮釈放、そして時には判決に至るまで、さまざまな場面でこのスコアを利用しています。
プロパブリカの記者は、フロリダ州ブロワード郡のスコアを情報公開請求し、そのスコアと実際の犯罪歴を照合して、2年以内に実際に再犯したかどうかを調べました。
分析したところ、黒人の被告人は白人よりも高いリスクスコアを割り当てられる傾向があり、2年後に再逮捕されていないのに、誤って高リスクと判定される可能性が高いことがわかりました（Larson et al.2016）。
刑事司法制度におけるスコアリングは、アルゴリズムが社会に展開されている一つの領域にすぎません。
「Machine Bias」シリーズではその後、Facebookの広告ターゲティングシステム、地理的に差別的な自動車保険料、Amazon.comの不公平な価格設定慣行など、あらゆる分野を取り上げています。
アルゴリズムによる意思決定は、官民を問わず、ますます浸透しています。
クレジットや保険のリスク評価、雇用システム、福祉管理、教育や教師のランキング、オンラインメディアのキュレーションなどの領域で目にすることができます（Eubanks, 2018; O'Neil, 2016; Pasquale, 2015）。
大規模に運用され、多くの人々に影響を与えるアルゴリズムは、時には論争の対象となる計算、ランキング、分類、関連付け、フィルタリングを行うことがあります。
膨大なデータに基づいて稼働するアルゴリズムは、社会で力を行使する新しい有力な方法となっています。
プロパブリカの「Machine Bias」シリーズが証明するように、アルゴリズムを通じて権力がどのように行使されるかを調査し、説明責任を果たすために、計算論的ジャーナリズムやデータ・ジャーナリズムの新たな分野が生まれています。
私はこのアルゴリズムを巡る説明責任報道を「ジャーナリズムの伝統的な監視機能を、アルゴリズムによって行使される権力に向け再調整したもの」と呼んでいます（Diakopoulos, 2015）。
アルゴリズムは、その客観性にもかかわらず、ミスを犯したり、バイアスをかけたりすることがあるため、綿密な調査が必要になります。
もちろん、アルゴリズ ムの説明責任には様々な形態があり、ジャーナ リズム以外にも、政治、法律、学術、活動家、芸術など、 多様な場で問われる可能性があります（Brain & Mattu, n.d.; Bucher, 2018）。
しかし、本章では、世論の圧力を動員することで説明責任を果たさせる、独立したジャーナリスティックな試みとしての報道に焦点を当てます。
これは、規制や法的基準の策定、市民社会における監査機関の創設、効果的な透明性政策の策定、反射的なアートショーの開催、学術的な批評の発表など、説明責任に貢献しうる他の手段を補完するものです。
ジャーナリズムにおける専門性を決めるには、まずアルゴリズムの何がニュースになるのかを定義することが重要です。
技術的には、アルゴリズムとは、特定の問題を解決するため、あるいは定義された結果を達成するために従う一連の手順です。情報プロセスの観点から言えば、アルゴリズムの結果は典型的な意思決定です。
アルゴリズムのパワーの核心は、コンピュータがこのような意思決定を非常に迅速かつ大規模に行い、多数の人々に影響を与えることができるという点にあります。
しかし、アルゴリズムの説明責任は、技術的側面にとどまりません。
アルゴリズムは複雑な社会工学的システムにおける設計者、運用者、所有者、管理者などの人々が織り成す技術の複合体として理解すべきなのです（Ananny, 2015; Seaver, 2017）。
アルゴリズムの説明責任とは、それらの人々がシステムの中で、またシステムを通じてどのように権力を行使し、システムの決定に最終的に責任を負うのかを理解することです。
多くの場合、アルゴリズムがニュースになるのは、アルゴリズムが何らかの形で「悪い」決定を下したときです。
これは、アルゴリズムが想定外のことをしたり、あるいは想定していたことをしなかったりすることを意味します。ジャーナリズムにとっては、間違った判断の社会的意義と結果が重要な要素となります。
個人にとって、あるいは社会にとって、どのような損害が生じる可能性があるのか。間違った決定は、個人に直接影響を与えるかもしれませんし、全体としては構造的なバイアスを強化するかもしれません。
また、間違った決定はコストがかかることもあります。ここでは、さまざまな誤った判断がニュースになった例を見てみましょう。
アルゴリズムのアングル
ここ数年、ジャーナリズムの世界で展開されてきたアルゴリズム・ビートを観察し、また私自身がアルゴリズムを調査する中で、アルゴリズムの説明責任を巡る議論に通底すると思われる、少なくとも4つの原動力を特定しました。
(a) 差別や不公平、(b) 予測や分類におけるエラーやミス、(c) 法律や社会的規範の違反、(d) 意図的または不注意によるアルゴリズムの誤用です。以下に、それぞれの例を挙げて説明します。
差別と不公平 差別と不公平の暴露は、アルゴリズムによる説明責任報道に共通するテーマです。
本章の冒頭で紹介したProPublica社の記事は、アルゴリズムがいかに異なるグループの人々の扱いに体系的な格差をもたらすかをはっきり示しました。
リスク評価のスコアを作成したNorthpointe社（現在はEquivant社に社名変更）は、このスコアは人種に関係なく同じ精度であり、それゆえ公平であると主張しました。
しかし、彼らの言う公平性は、黒人に影響を与えるミスの数が不均衡であることを考慮に入れていませんでした。
差別や不公平の話は、適用される公平性の定義に依存し、異なる政治的仮定を反映している可能性があるのです（Lepri et al.、2018）。
私は，アルゴリズムシステムによる不公平さを明らかにするストーリーにも取り組んできました．特に，Uberのダイナミック・プライシングがワシントンDCの近隣地域ではどう違うのかを調べました（Stark & Diakopoulos, 2016）。
待ち時間の違いや，Uberのサージ・プライシング・アルゴリズムに基づいて待ち時間がどのように変化するかがまず見て取れたため、地域によってサービス品質（すなわち待ち時間）が異なるという仮説を立てました。
異なる国勢調査区の待ち時間を系統的にサンプリングすることで、所得、貧困率、近隣の人口密度などの他の要因をコントロールしても、有色人種が多い国勢調査区は車の待ち時間が長くなる傾向にあることを示しました。
Uberのドライバーの行動や潜在的なバイアスなど、他の人的要因もシステムに影響を与えるため、この不公平な結果をUberの技術的アルゴリズムのみに帰することは困難です。
しかし、この結果は、全体として考えると、システムには人口統計に関連した格差があることを示唆しています。
エラーとミス アルゴリズムは、分類、予測、フィルタリングの決定において、特定のエラーやミスを犯した場合にもニュースになります。
FacebookやGoogleなどのプラットフォームでは、アルゴリズムによるフィルタリングによって、ヘイトスピーチ、暴力、ポルノなどの有害なコンテンツへの接触を減らしていることを考えてみましょう。
これは、特に子どもにとって安全であるとうたって販売されている製品（GoogleのYouTube Kidsなど）では、子どものような特定の脆弱な集団を保護するために重要となる場合があります。
このアプリのフィルタリングアルゴリズムのエラーは、子どもたちが不適切だったり暴力的なコンテンツに遭遇することがあるため、ニュースになるのです（Maheshwari, 2017）。
古典的に、アルゴリズムは2種類のミスを犯します。偽陽性と偽陰性です。
YouTube Kidsの場合、誤検出とは、実際には子供にとって全く問題ないのに、不適切と誤って分類された動画のことです。偽陰性とは、適切と分類された動画が、本当は子供に見せたくないものであることです。
分類の決定は、個人が受けるポジティブないしネガティブな待遇の増減に影響します。
アルゴリズムが誤って個人を選んでアイスクリームを無料で提供しても（ポジティブな扱いの増加）、その個人が文句を言うことはありません（ただし、他の人は不公平だと言うかもしれません）。
一般的にエラーがニュースになるのは、子供に不適切なビデオを見せてしまうなど、その人にとってネガティブな扱いを増やすことにつながる場合です。また、機会を逃すなど、個人に対するプラスの扱いが減少した場合もニュースになります。
アルゴリズムが誤って除外したため、資格があるのに特別なオファーを受けることができない人がいたとしてみましょう。
最後に、エラーがニュースになるのは、正当化されるべきネガティブな注目を減少させる原因となる場合です。例えば、犯罪リスク評価のアルゴリズムが、高リスクの人物を誤って低リスクと判定してしまうことを考えてみましょう（偽陰性）。
本人にとっては良いことですが、再び犯罪を犯す可能性のある人を自由にすることで、公共の安全に大きなリスクをもたらします。
法的・社会的規範の違反。予測アルゴリズムは、時として確立された法的・社会的規範の境界線を試すことがあり、それが別の機会や報道の角度につながることがあります。
アルゴリズムによる名誉毀損の可能性について考えてみましょう（Diakopoulos, 2013; Lewis et al.2019）。
名誉毀損とは、「ある人物を憎悪、嘲笑、軽蔑の対象とし、同業者の尊敬を低下させ、敬遠される原因となり、または事業や取引において損害を与える虚偽の事実の陳述」と定義されています。
ここ数年、Googleの（検索語句のタイプ中に自動的に検索候補を表示する）オートコンプリートアルゴリズムによって名誉を毀損されたと感じる個人を巡り、数多くの記事や訴訟が発生しています。
オートコンプリートは、個人や企業の名前を、犯罪や詐欺、破産、性的行為などあらゆるものと結び付け、その人の評判に影響を与える可能性があります。
アルゴリズムは、プライバシーなどの社会的規範を侵害した場合にもニュースになります。
（テックサイトの）GizmodoはFacebookの「People You May Know」（PYMK）アルゴリズムを大きく取り上げています。PYMKアルゴリズムは、プラットフォーム上で潜在的な「友達」を提案しますが、それが時に不適切であったり、望ましくないものであったりします（Hill, 2017b）。
ある記事では、記者が、PYMKが風俗嬢の本当の身元を客に暴露したケースを確認しました（Hill, 2017a）。
これは、性風俗につきまとう付随する潜在的な偏見だけでなく、ストーカーになりかねない客への恐怖（という観点）からも問題となります。
名誉毀損やプライバシー侵害は、ここで考えられる二つの視点に過ぎません。
ジャーナリストは、アルゴリズムが様々な社会的文脈の中で生み出す、他の様々な法的・社会的規範の侵害に注意を払う必要があります。
アルゴリズムは、測定可能なものだけをデータとして取り込む定量化された現実に依存しているため、正確な判断を下すために不可欠な社会的・法的な背景の多くを見逃してしまう可能性があります。
特定のアルゴリズムが実際に世界をどのように「見ている」のかを理解することで、判断の裏付けとなる欠落した部分を明らかにし、批判につなげることができます。
人間による誤用。アルゴリズムによる意思決定は、多くの場合、社会工学的なシステムに組み込まれた人間とアルゴリズムの組み合わせによる、より大きな意思決定プロセスに組み込まれています。
アルゴリズムの微妙な技術的構成要素のいくつかはアクセス不能であるにもかかわらず,アルゴリズムの社会工学的性質は,ユーザー、デザイナー、所有者、その他のステークホルダーがシステム全体に対して持つ関係性を調査するための新たな機会をもたらします（Trielli & Diakopoulos, 2017）。
社会工学に関係する人々がアルゴリズムを誤用しているなら、これもまたニュース価値があるかもしれません。
アルゴリズムの設計者は、システムの合理的な使用状況を予測してガイドライン定めており、使う人々がこれらを無視すると、過失や誤用のストーリーにつながる可能性があります。
プロパブリカのリスクアセスメントの記事は、その典型的な例です。Northpointe社は、男性用と女性用の2種類の(再犯リスクを予測する)ツールを作成していました。
統計モデルは、それが使用される人口を反映したデータで訓練される必要があり、性別は再犯率の予測において重要な要素です。しかし（フロリダ州）ブロワード郡は、男性用に設計・校正されたリスクスコアを、女性用にも使用して誤用していたのです（Larson, 2016）。
アルゴリズムを調査する方法

アルゴリズムの力を調査するには様々なルートがあり、常に一つのアプローチが適切とは限りません。
高度な技術を要するリバースエンジニアリングや（人の目でコードを検証する）コードインスペクションの手法、自動化されたデータやクラウドソースによるデータ収集を利用した監査、さらにはアルゴリズムの反応に基づいた試作や批評を行うローテクなアプローチまで、さまざまな手法が登場しています（Diakopoulos, 2017, 2019）。
各記事は、アルゴリズム、そのデータおよびそのコードへのアクセスが可能であるかを含め、視座と文脈に応じて異なるアプローチを必要とする場合があります。
組織的な差別について暴露する記事は、オンラインで収集したデータを使った監査手法をとり、一方で、意図したポリシーが正しく実施されているかどうかを検証するにはコードレビューが必要になるかもしれません（Lecher, 2018）。
デザイナー、開発者、データサイエンティストなどの企業内部の人間に話を聞くための伝統的なジャーナリストが行ってきた情報収集や、公文書請求をして影響を受けた個人を探し出すことは、これまでと同様に重要です。
この短い章でこれらの手法のすべてを深く説明することはできませんが、せめてここでは、ジャーナリストが監査を使ってアルゴリズムを調査する方法についてもう少し詳しく説明したいと思います。
監査技術は、住宅市場などのシステムにおける社会的バイアスの研究に何十年も前から用いられており、最近ではアルゴリズムの研究にも応用されています（Gaddis, 2017; Sandvig et al, 2014）。
基本的な考え方は、アルゴリズムへの入力を十分に異なる方法で変化させ、出力をモニターすれば、入力と出力を相関させて、アルゴリズムがどのように機能しているかの理論を構築できるというものです（Diakopoulos, 2015）。
ある入力に対してアルゴリズムが（法や社会規範に）反する出力をしたならば、エラーを集計し、体系的に偏っているかどうかを確認するのに役立ちます。
アルゴリズムがAPIやオンラインのWebページを介してアクセスできる場合、出力データを自動的に収集することができます（Valentino-DeVries et al）。
パーソナライズされたアルゴリズムに対応するため、アルゴリズムが独自の「視点」を適用する可能性があるさまざまな人からデータを収集するクラウドソーシングと、監査技術を結びつけることもできます。
ドイツのAlgorithmWatchは、この手法をGoogle検索結果のパーソナライズの研究に効果的に利用し、ブラウザのプラグインを介してデータを共有した4,000人以上のユーザーから約600万件の検索結果を収集しました（本書のChristina Elmerの章で詳しく説明されています）。
ユーザーは自分のコンピュータにソフトウェアをダウンロードし、PYMK（知り合いかも機能）の結果を定期的に追跡します。ローカルなのでプライバシーは保たれます。
そして記者は、自分の結果が気になる、あるいは意外だと思うユーザーから（報道の）ヒントを募ることができます（Hill & Mattu, 2018）。
アルゴリズムを監査することは、気の弱い人にはできません。情報が不足していると、どこから始めればいいのか、何を求めればいいのか、結果をどう解釈すればいいのか、アルゴリズムの挙動に見られるパターンをどう説明すればいいのかさえ分からないことがあります。
また、アルゴリズムに期待されることを知り、それを定義すること、そしてその期待が文脈によって、また世界的に異なる道徳的、社会的、文化的、法的な基準や規範によってどのように変化するかという課題もあります。
例えば、犯罪リスクを評価するアルゴリズムと、航空座席の値段を変えて請求するアルゴリズムとでは、公平性に対する期待が異なるかもしれません。
ニュースになるようなミスや偏りを特定するためには、まず正常や偏りのない状態とはどのようなものかを定義する必要があります。
2016年の米国選挙の際にGoogleの検索結果のニュースソースを監査したときのように、データ駆動型（報道）の基準に基づいて定義されることもあります（Diakopoulos et al）。
アルゴリズムに関する情報への法的アクセスの問題も出てきますが、これはもちろん司法権に大きく左右されます（Bhandari & Goodman, 2017）。
米国では、情報自由法（FOI）により、国民の政府文書へのアクセスが規定されていますが、アルゴリズムに関する文書に対する各機関の対応は、よく言ってもバラバラです（Brauneis & Goodman, 2018; Diakopoulos, 2016; Fink, 2017を参照）。
アルゴリズムに関する情報への一般のアクセスがより容易になるように、法改正が必要かもしれません。
情報不足、明確化しにくい期待、不確実な法的アクセスが完全に道をふさいでいるのではないのなら、アルゴリズムが非常に気まぐれなものでもあることを思い出してください。
今日のバージョンのアルゴリズムは、昨日のものとはすでに違うかもしれません。例えば、Googleは通常、年に500～600回、検索アルゴリズムを変更しています。
アルゴリズムがどのように変化し、進化しているのかを理解するためには、その変化の大きさに応じて、時間をかけてモニタリングする必要があると言って良いでしょう。

Recommendations Moving Forward

To get started and make the most of algorithmic accountability reporting, I would recommend three things. Firstly, we have developed a resource called Algorithm Tips, which curates relevant methods, examples and educational resources, and hosts a database of algorithms for potential investigation (first covering algorithms in the US federal government and then expanded to cover more jurisdictions globally).7 If you are looking for resources to learn more and help to get a project off the ground, that could be one starting point (Trielli et al., 2017). Secondly, focus on the outcomes and impacts of algorithms rather than trying to explain the exact mechanism of their decision making. Identifying algorithmic discrimination (i.e., an output) oftentimes has more value to society as an initial step than explaining exactly how that discrimination came about. By focusing on outcomes, journalists can provide a first-order diagnostic and signal an alarm which other stakeholders can then dig into in other accountability forums. Finally, much of the published algorithmic accountability reporting I have cited here is done in teams, and with good reason. Effective algorithmic accountability reporting demands all of the traditional skills journalists need in reporting and interviewing, domain knowledge of a beat, public records requests and analysis of the returned documents, and writing results clearly and compellingly, while often also relying on a host of new capabilities like scraping and cleaning data, designing audit studies, and using advanced statistical techniques. Expertise in these different areas can be distributed among a team, or with external collaborators, as long as there is clear communication, awareness and leadership. In this way, methods specialists can partner with different domain experts to understand algorithmic power across a larger variety of social domains.


Footnotes

1. www.propublica.org/series/machine-bias

2. The term algorithmic accountability was originally coined in: Diakopoulos, N. (2013, August 2). Sex, violence, and autocomplete algorithms. Slate Magazine. slate.com/technology/2013/08/words-banned-from-bing-and-googles-autocomplete-algorithms.html technology/2013/08/words-banned-from-bing-and-googles-autocomplete-algorithms.html; and elaborated in: Diakopoulos, N. (2013, October 3). Rage against the algorithms. The Atlantic. www.theatlantic.com/technology/archive/2013/10/rage-against-the-algorithms/280255/

3. For an activist/artistic frame, see: Brain, T., & Mattu, S. (n.d.). Algorithmic disobedience. samatt.github.io/algorithmic-disobedience/#/. For an academic treatment examining algorithmic power, see: Bucher, T. (2018). If . . . then: Algorithmic power and politics. Oxford University Press. A broader selection of the academic scholarship on critical algorithm studies can be found here: socialmediacollective.org/reading-lists/critical-algorithm-studies

4.www.dmlp.org/legal-guide/defamation

5. For more a more complete treatment of methodological options, see: Diakopoulos, N. (2019). Automating the news: How algorithms are rewriting the media. Harvard University Press; see also: Diakopoulos, N. (2017). Enabling accountability of algorithmic media: Transparency as a constructive and critical lens. In T. Cerquitelli, D. Quercia, & F. Pasquale (Eds.), Transparent data mining for big and small data (pp. 25–43). Springer International Publishing.doi.org/10.1007/978-3-319-54024-5_2

6. algorithmwatch.org/de/filterblase-geplatzt-kaum-raum-fuer-personalisierung-bei-google-suchen-zur-bundestagswahl-2017/ (German language)

7. algorithmtips.org

Works Cited

Ananny, M. (2015). Toward an ethics of algorithms. Science, Technology & Human Values, 41(1), 93–117.

Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016, May 23). Machine bias. ProPublica. www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

Bhandari, E., & Goodman, R. (2017). Data journalism and the computer fraud and abuse act: Tips for moving forward in an uncertain landscape. Computation+Journalism Symposium. www.aclu.org/other/data-journalism-and-computer-fraud-and-abuse-act-tips-moving-forward-uncertain-landscape

Brain, T., & Mattu, S. (n.d.). Algorithmic disobedience. samatt.github.io/algorithmic-disobedience

Brauneis, R., & Goodman, E. P. (2018). Algorithmic transparency for the smart city.

Yale Journal of Law & Technology, 20, 103–176.

Bucher, T. (2018). If . . . then: Algorithmic power and politics. Oxford University Press. Diakopoulos, N. (2013, August 6). Algorithmic defamation: The case of the shameless autocomplete. Tow Center for Journalism. towcenter.org/algorithmic-defamation-the-case-of-the-shameless-autocomplete

Diakopoulos, N. (2015). Algorithmic accountability: Journalistic investigation ofcomputational power structures. Digital Journalism, 3(3), 398–415. doi.org/10.1080/21670811.2014.976411

Diakopoulos, N. (2016, May 24). We need to know the algorithms the govern- ment uses to make important decisions about us. The Conversation. theconversation.com/we-need-to-know-the-algorithms-the-government-uses-to-make-important-decisions-about-us-57869

Diakopoulos, N. (2017) Enabling Accountability of Algorithmic Media: Transparency as a Constructive and Critical Lens. In T. Cerquitelli, D. Quercia, & F. Pasquale (Eds.), Transparent data mining for Big and Small Data (pp. 25–44). Springer.

Diakopoulos, N. (2019). Automating the News: How Algorithms are Rewriting the Media. Harvard University Press.

Diakopoulos, N., Trielli, D., Stark, J., & Mussenden, S. (2018). I vote for—How search informs our choice of candidate. In M. Moore & D. Tambini (Eds.), Digital Domi- nance: The power of Google, Amazon, Facebook, and Apple (pp. 320–341). Oxford University Press. www.academia.edu/37432634/I_Vote_For_How_Search_ Informs_Our_Choice_of_Candidate

Eubanks, V. (2018). Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin’s Press.

Fink, K. (2017). Opening the government’s black boxes: Freedom of information and algorithmic accountability. Digital Journalism, 17(1).doi.org/10.1080/1369118X.2017.1330418

Gaddis, S. M. (2017). An introduction to audit studies in the social sciences. In M. Gaddis (Ed.), Audit studies: Behind the scenes with theory, method, and nuance (pp. 3–44). Springer International Publishing.

Gillespie, T., & Seaver, N. (2015, November 5). Critical algorithm studies: A reading list. Social Media Collective. socialmediacollective.org/reading-lists/

Hill, K. (2017a, October). How Facebook outs sex workers. Gizmodo. gizmodo.com/how-facebook-outs-sex-workers-1818861596

Hill, K. (2017b, November). How Facebook f igures out everyone you’ve ever met. Gizmodo. gizmodo.com/how-facebook-figures-out-everyone-youve-ever-met-1819822691

Hill, K., & Mattu, S. (2018, January 10). Keep track of who Facebook thinks you know with this nifty tool. Gizmodo. gizmodo.com/keep-track-of-who- facebook-thinks-you-know-with-this-ni-1819422352

Larson, J. (2016, October 20). Machine bias with Jeff Larson [Data Stories podcast]. datastori.es/85-machine-bias-with-jeff-larson/

Larson, J., Mattu, S., Kirchner, L., & Angwin, J. (2016, May 23). How we analyzed the COMPAS recidivism algorithm. ProPublica.www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm

Lecher, C. (2018, March 21). What happens when an algorithm cuts your health care. The Verge. www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy

Lepri, B., Oliver, N., Letouzé, E., Pentland, A., & Vinck, P. (2018). Fair, transparent, and accountable algorithmic decision-making processes. Philosophy & Technology, 31(4), 611–627. https://doi.org/10.1007/s13347...

Lewis, S. C., Sanders, A. K., & Carmody, C. (2019). Libel by algorithm? Automated journalism and the threat of legal liability. Journalism and Mass Communication Quarterly, 96(1), 60–81. https://doi.org/10.1177/107769...

Maheshwari, S. (2017, November 4). On Youtube Kids, startling videos slip past f ilters. The New York Times. www.nytimes.com/2017/11/04/business/media/youtube-kids-paw-patrol.html

O’Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books.

Pasquale, F. (2015). The black box society: The secret algorithms that control money and information. Harvard University Press.

Sandvig, C., Hamilton, K., Karahalios, K., & Langbort, C. (2014, May 22). Audit- ing algorithms: Research methods for detecting discrimination on Internet platforms. International Communication Association preconference on Data and Discrimination Converting Critical Concerns into Productive Inquiry, Seattle, WA.

Seaver, N. (2017). Algorithms as culture: Some tactics for the ethnography of algo- rithmic systems. Big Data & Society, 4(2). https://doi.org/10.1177/205395...

Stark, J., & Diakopoulos, N. (2016, March 10). Uber seems to offer better service in areas with more White people. That raises some tough questions. The Washington Post. www.washingtonpost.com/news/wonk/wp/2016/03/10/uber-seems-to-offer-better-service-in-areas-with-more-white-people-that-raises-some-tough-questions/

Trielli, D., & Diakopoulos, N. (2017, May 30). How to report on algorithms even if you’re not a data whiz. Columbia Journalism Review. www.cjr.org/%20tow_center/algorithms-reporting-algorithmtips.php

Trielli, D., Stark, J., & Diakopoulos, N. (2017). Algorithm tips: A resource for algorithmic accountability in Government. Computation + Journalism Symposium.

Valentino-DeVries, J., Singer-Vine, J., & Soltani, A. (2012, December 24). Websites vary prices, deals based on users’ information. The Wall Street Journal. https:// www.wsj.com/articles/SB10001424127887323777204578189391813881534
